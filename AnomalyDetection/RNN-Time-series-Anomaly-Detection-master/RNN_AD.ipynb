{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN网络进行时序数据的异常检测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "### 首先导入必要的库\n",
    "requests：用于发送HTTP请求。\n",
    "\n",
    "os 和 pathlib：用于文件和目录操作。\n",
    "\n",
    "pickle：用于序列化和反序列化Python对象结构。\n",
    "\n",
    "shutil：用于文件操作，如解压缩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from shutil import unpack_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据集的URL\n",
    "> urls 字典通常指的是一个用于存储URL（Uniform Resource Locator，统一资源定位符）的字典（Dictionary）数据结构。\n",
    "\n",
    "urls 字典被用来存储不同数据集的下载链接。每个键（如 'ecg'、'gesture'、'space_shuttle' 等）对应一个特定的数据集，而每个键的值则是一个包含该数据集所有相关文件下载链接的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = dict()\n",
    "urls['ecg']=['http://www.cs.ucr.edu/~eamonn/discords/ECG_data.zip',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/mitdbx_mitdbx_108.txt',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/qtdbsele0606.txt',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/chfdbchf15.txt',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/qtdbsel102.txt']\n",
    "urls['gesture']=['http://www.cs.ucr.edu/~eamonn/discords/ann_gun_CentroidA']\n",
    "urls['space_shuttle']=['http://www.cs.ucr.edu/~eamonn/discords/TEK16.txt',\n",
    "                       'http://www.cs.ucr.edu/~eamonn/discords/TEK17.txt',\n",
    "                       'http://www.cs.ucr.edu/~eamonn/discords/TEK14.txt']\n",
    "urls['respiration']=['http://www.cs.ucr.edu/~eamonn/discords/nprs44.txt',\n",
    "                     'http://www.cs.ucr.edu/~eamonn/discords/nprs43.txt']\n",
    "urls['power_demand']=['http://www.cs.ucr.edu/~eamonn/discords/power_data.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载并处理数据集：\n",
    "#### 下载数据集\n",
    "遍历 urls 字典中的每个数据集名称。\n",
    "\n",
    "对于每个数据集，创建一个目录来存放原始数据（raw_dir）。\n",
    "\n",
    "遍历每个数据集的URL列表，下载文件到指定目录，并根据文件类型（如zip或txt）进行处理。\n",
    "\n",
    "#### 处理数据集\n",
    "对于每个下载的文本文件，读取内容并进行处理。\n",
    "对于 'ecg' 数据集，移除时间步通道。\n",
    "对于特定的文件，根据条件标记异常点（anomaly points），并添加到数据列表中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/ECG_data.zip\n",
      "Saving to dataset\\ecg\\raw\\ECG_data.txt\n",
      "Extracting to dataset\\ecg\\raw\\ECG_data.zip\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/mitdbx_mitdbx_108.txt\n",
      "Saving to dataset\\ecg\\raw\\mitdbx_mitdbx_108.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/qtdbsele0606.txt\n",
      "Saving to dataset\\ecg\\raw\\qtdbsele0606.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/chfdbchf15.txt\n",
      "Saving to dataset\\ecg\\raw\\chfdbchf15.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/qtdbsel102.txt\n",
      "Saving to dataset\\ecg\\raw\\qtdbsel102.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/ann_gun_CentroidA\n",
      "Saving to dataset\\gesture\\raw\\ann_gun_CentroidA.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/TEK16.txt\n",
      "Saving to dataset\\space_shuttle\\raw\\TEK16.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/TEK17.txt\n",
      "Saving to dataset\\space_shuttle\\raw\\TEK17.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/TEK14.txt\n",
      "Saving to dataset\\space_shuttle\\raw\\TEK14.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/nprs44.txt\n",
      "Saving to dataset\\respiration\\raw\\nprs44.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/nprs43.txt\n",
      "Saving to dataset\\respiration\\raw\\nprs43.txt\n",
      "Downloading http://www.cs.ucr.edu/~eamonn/discords/power_data.txt\n",
      "Saving to dataset\\power_demand\\raw\\power_data.txt\n"
     ]
    }
   ],
   "source": [
    "for dataname in urls:\n",
    "    # 创建原始文件存放地址/dataset/数据集名称/raw。\n",
    "    raw_dir = Path('dataset', dataname, 'raw')\n",
    "    # 创建这个路径所指向的目录。parents=True参数意味着如果父目录不存在，也会一并创建。exist_ok=True参数意味着如果目录已经存在，不会抛出异常。\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for url in urls[dataname]:\n",
    "        filename = raw_dir.joinpath(Path(url).name)\n",
    "        print('Downloading', url)\n",
    "        # 发送HTTP GET请求并下载文件\n",
    "        resp =requests.get(url)\n",
    "        filename.write_bytes(resp.content)\n",
    "        if filename.suffix=='':\n",
    "            filename.rename(filename.with_suffix('.txt'))\n",
    "        print('Saving to', filename.with_suffix('.txt'))\n",
    "        # 如果文件是ZIP格式（.zip），则打印解压缩的信息\n",
    "        if filename.suffix=='.zip':\n",
    "            print('Extracting to', filename)\n",
    "            unpack_archive(str(filename), extract_dir=str(raw_dir))\n",
    "\n",
    "\n",
    "    for filepath in raw_dir.glob('*.txt'):\n",
    "        with open(str(filepath)) as f:\n",
    "            # Label anomaly points as 1 in the dataset\n",
    "            labeled_data=[]\n",
    "            # 如果当前处理的数据文件的父目录名称是ecg，则从tokens列表中移除第一个元素。这可能是为了移除某种不需要的时间步长或通道信息。\n",
    "            for i, line in enumerate(f):\n",
    "                tokens = [float(token) for token in line.split()]\n",
    "                if raw_dir.parent.name== 'ecg':\n",
    "                    # Remove time-step channel\n",
    "                    tokens.pop(0)\n",
    "                if filepath.name == 'chfdbchf15.txt':\n",
    "                    tokens.append(1.0) if 2250 < i < 2400 else tokens.append(0.0)\n",
    "                elif filepath.name == 'xmitdb_x108_0.txt':\n",
    "                    tokens.append(1.0) if 4020 < i < 4400 else tokens.append(0.0)\n",
    "                elif filepath.name == 'mitdb__100_180.txt':\n",
    "                    tokens.append(1.0) if 1800 < i < 1990 else tokens.append(0.0)\n",
    "                elif filepath.name == 'chfdb_chf01_275.txt':\n",
    "                    tokens.append(1.0) if 2330 < i < 2600 else tokens.append(0.0)\n",
    "                elif filepath.name == 'ltstdb_20221_43.txt':\n",
    "                    tokens.append(1.0) if 650 < i < 780 else tokens.append(0.0)\n",
    "                elif filepath.name == 'ltstdb_20321_240.txt':\n",
    "                    tokens.append(1.0) if 710 < i < 850 else tokens.append(0.0)\n",
    "                elif filepath.name == 'chfdb_chf13_45590.txt':\n",
    "                    tokens.append(1.0) if 2800 < i < 2960 else tokens.append(0.0)\n",
    "                elif filepath.name == 'stdb_308_0.txt':\n",
    "                    tokens.append(1.0) if 2290 < i < 2550 else tokens.append(0.0)\n",
    "                elif filepath.name == 'qtdbsel102.txt':\n",
    "                    tokens.append(1.0) if 4230 < i < 4430 else tokens.append(0.0)\n",
    "                elif filepath.name == 'ann_gun_CentroidA.txt':\n",
    "                    tokens.append(1.0) if 2070 < i < 2810 else tokens.append(0.0)\n",
    "                elif filepath.name == 'TEK16.txt':\n",
    "                    tokens.append(1.0) if 4270 < i < 4370 else tokens.append(0.0)\n",
    "                elif filepath.name == 'TEK17.txt':\n",
    "                    tokens.append(1.0) if 2100 < i < 2145 else tokens.append(0.0)\n",
    "                elif filepath.name == 'TEK14.txt':\n",
    "                    tokens.append(1.0) if 1100 < i < 1200 or 1455 < i < 1955 else tokens.append(0.0)\n",
    "                elif filepath.name == 'nprs44.txt':\n",
    "                    tokens.append(1.0) if 16192 < i < 16638 or 20457 < i < 20911 else tokens.append(0.0)\n",
    "                elif filepath.name == 'nprs43.txt':\n",
    "                    tokens.append(1.0) if 12929 < i < 13432 or 14877 < i < 15086 or 15729 < i < 15924 else tokens.append(0.0)\n",
    "                elif filepath.name == 'power_data.txt':\n",
    "                    tokens.append(1.0) if 8254 < i < 8998 or 11348 < i < 12143 or 33883 < i < 34601 else tokens.append(0.0)\n",
    "                labeled_data.append(tokens)\n",
    "\n",
    "            # Fill in the point where there is no signal value.\n",
    "            if filepath.name == 'ann_gun_CentroidA.txt':\n",
    "                for i, datapoint in enumerate(labeled_data):\n",
    "                    for j,channel in enumerate(datapoint[:-1]):\n",
    "                        if channel == 0.0:\n",
    "                            labeled_data[i][j] = 0.5 * labeled_data[i - 1][j] + 0.5 * labeled_data[i + 1][j]\n",
    "\n",
    "            # Save the labeled dataset as .pkl extension\n",
    "            labeled_whole_dir = raw_dir.parent.joinpath('labeled', 'whole')\n",
    "            labeled_whole_dir.mkdir(parents=True, exist_ok=True)\n",
    "            with open(str(labeled_whole_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                pickle.dump(labeled_data, pkl)\n",
    "\n",
    "            # Divide the labeled dataset into trainset and testset, then save them\n",
    "            labeled_train_dir = raw_dir.parent.joinpath('labeled','train')\n",
    "            labeled_train_dir.mkdir(parents=True,exist_ok=True)\n",
    "            labeled_test_dir = raw_dir.parent.joinpath('labeled','test')\n",
    "            labeled_test_dir.mkdir(parents=True,exist_ok=True)\n",
    "            if filepath.name == 'chfdb_chf13_45590.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[:2439], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[2439:3726], pkl)\n",
    "            elif filepath.name == 'chfdb_chf01_275.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[:1833], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[1833:3674], pkl)\n",
    "            elif filepath.name == 'chfdbchf15.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[3381:14244], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[33:3381], pkl)\n",
    "            elif filepath.name == 'qtdbsel102.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[10093:44828], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[211:10093], pkl)\n",
    "            elif filepath.name == 'mitdb__100_180.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[2328:5271], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[73:2328], pkl)\n",
    "            elif filepath.name == 'stdb_308_0.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[2986:5359], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[265:2986], pkl)\n",
    "            elif filepath.name == 'ltstdb_20321_240.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[1520:3531], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[73:1520], pkl)\n",
    "            elif filepath.name == 'xmitdb_x108_0.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[424:3576], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[3576:5332], pkl)\n",
    "            elif filepath.name == 'ltstdb_20221_43.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[1121:3731], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[0:1121], pkl)\n",
    "            elif filepath.name == 'ann_gun_CentroidA.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[3000:], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[:3000], pkl)\n",
    "            elif filepath.name == 'nprs44.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[363:12955], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[12955:24082], pkl)\n",
    "            elif filepath.name == 'nprs43.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[4285:10498], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[10498:17909], pkl)\n",
    "            elif filepath.name == 'power_data.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[15287:33432], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[501:15287], pkl)\n",
    "            elif filepath.name == 'TEK17.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[2469:4588], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[1543:2469], pkl)\n",
    "            elif filepath.name == 'TEK16.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[521:3588], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[3588:4539], pkl)\n",
    "            elif filepath.name == 'TEK14.txt':\n",
    "                with open(str(labeled_train_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[2089:4098], pkl)\n",
    "                with open(str(labeled_test_dir.joinpath(filepath.name).with_suffix('.pkl')), 'wb') as pkl:\n",
    "                    pickle.dump(labeled_data[97:2089], pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存处理后的数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集的文件路径\n",
    "nyc_taxi_raw_path = Path('dataset/nyc_taxi/raw/nyc_taxi.csv')\n",
    "# nyc_taxi_raw_path.mkdir(parents=True, exist_ok=True)\n",
    "# 创建一个空列表 labeled_data，用于存储处理后的每一行数据及其对应的标签\n",
    "labeled_data = []\n",
    "\n",
    "with open(str(nyc_taxi_raw_path),'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # 对每一行，使用 strip() 方法去除首尾空白字符，再使用 split(',') 方法以逗号为分隔符分割字符串\n",
    "        # line.strip().split(',')[1:] 跳过第一列（时间）\n",
    "        # 转换为浮点数，并将它们作为列表赋值给 tokens\n",
    "        tokens = [float(token) for token in line.strip().split(',')[1:]]\n",
    "        # 如果 i 在以下任一范围内，则在 tokens 列表的末尾添加 1\n",
    "        tokens.append(1) if 150 < i < 250 or   \\\n",
    "                            5970 < i < 6050 or \\\n",
    "                            8500 < i < 8650 or \\\n",
    "                            8750 < i < 8890 or \\\n",
    "                            10000 < i < 10200 or \\\n",
    "                            14700 < i < 14800 \\\n",
    "                          else tokens.append(0)\n",
    "        labeled_data.append(tokens)\n",
    "# >划分train\n",
    "# 生成一个新的文件路径，并将先前处理的 labeled_data 列表的前 13104 个元素，以二进制格式保存为一个 .pkl 文件\n",
    "nyc_taxi_train_path = nyc_taxi_raw_path.parent.parent.joinpath('labeled','train',nyc_taxi_raw_path.name).with_suffix('.pkl')\n",
    "# 创建目录（如果不存在）\n",
    "nyc_taxi_train_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# 使用 pickle 保存数据\n",
    "with open(str(nyc_taxi_train_path),'wb') as pkl:\n",
    "    pickle.dump(labeled_data[:13104], pkl)\n",
    "# >划分test\n",
    "nyc_taxi_test_path = nyc_taxi_raw_path.parent.parent.joinpath('labeled','test',nyc_taxi_raw_path.name).with_suffix('.pkl')\n",
    "nyc_taxi_test_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(str(nyc_taxi_test_path),'wb') as pkl:\n",
    "    pickle.dump(labeled_data[13104:], pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以上数据下载和处理部分完毕**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarin/Dev/Test\n",
    "实现了一个循环神经网络（RNN）模型来处理时间序列数据集。脚本包含了模型的训练、评估和预测。\n",
    "### 导入必要的库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import preprocess_data\n",
    "from model import model\n",
    "from torch import optim\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from anomalyDetector import fit_norm_distribution_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析命令行参数\n",
    "使用argparse库来解析命令行参数，这些参数控制着模型的训练和行为。例如，数据集类型、模型类型、学习率等\n",
    "\n",
    "举个例子：`parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)')`\n",
    "                    \n",
    "添加一个名为 --model 的命令行参数 这样调用：`args.model`\n",
    "\n",
    "type=str 指定该参数的类型为字符串。\n",
    "\n",
    "default='LSTM' 指定当命令行中没有提供该参数时的默认值为 'LSTM'。\n",
    "\n",
    "help 参数提供了一个描述信息，用于显示帮助信息时解释该参数的用途。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch RNN Prediction Model on Time-series Dataset')\n",
    "parser.add_argument('--data', type=str, default='ecg',\n",
    "                    help='type of the dataset (ecg, gesture, power_demand, space_shuttle, respiration, nyc_taxi')\n",
    "parser.add_argument('--filename', type=str, default='chfdb_chf13_45590.pkl',\n",
    "                    help='filename of the dataset')\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)')\n",
    "parser.add_argument('--augment', type=bool, default=True,\n",
    "                    help='augment')\n",
    "parser.add_argument('--emsize', type=int, default=32,\n",
    "                    help='size of rnn input features')\n",
    "parser.add_argument('--nhid', type=int, default=32,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--res_connection', action='store_true',\n",
    "                    help='residual connection')\n",
    "parser.add_argument('--lr', type=float, default=0.0002,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--clip', type=float, default=10,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=400,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=64, metavar='N',\n",
    "                    help='eval_batch size')\n",
    "parser.add_argument('--bptt', type=int, default=50,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--teacher_forcing_ratio', type=float, default=0.7,\n",
    "                    help='teacher forcing ratio (deprecated)')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights (deprecated)')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--device', type=str, default='cuda',\n",
    "                    help='cuda or cpu')\n",
    "parser.add_argument('--log_interval', type=int, default=10, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save_interval', type=int, default=10, metavar='N',\n",
    "                    help='save interval')\n",
    "parser.add_argument('--save_fig', action='store_true',\n",
    "                    help='save figure')\n",
    "parser.add_argument('--resume','-r',\n",
    "                    help='use checkpoint model parameters as initial parameters (default: False)',\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('--pretrained','-p',\n",
    "                    help='use checkpoint model parameters and do not train anymore (default: False)',\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('--prediction_window_size', type=int, default=10,\n",
    "                    help='prediction_window_size')\n",
    "args = parser.parse_args()\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "TimeseriesData = preprocess_data.PickleDataLoad(data_type=args.data, filename=args.filename,\n",
    "                                                augment_test_data=args.augment)\n",
    "train_dataset = TimeseriesData.batchify(args,TimeseriesData.trainData, args.batch_size)\n",
    "test_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, args.eval_batch_size)\n",
    "gen_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FirstGirlfrend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
